\section{Background}
\subsection{State Model Inference}
Roughly speaking, dynamic EFSM\footnote{Extended Finite State Machines, are special kind of state machines that have conditional expressions called `transition guards' on their transitions. A state transition can only happen if the transition guard evaluates true.} inference algorithms generally take a trace of `events' (along with perhaps some variable values) as their input \cite{walkinshaw2016inferring} to infer a generalized finite state machine. They use the events to find the state transitions and the values for detecting invariants and generating guard  conditions for the transitions. 
k Tails, Gk Tail, EDSM, and MINT are examples of these algorithms, each improving upon the previous one \cite{biermann1972synthesis, lorenzoli2008automatic, lang1998results, walkinshaw2016inferring}.


\subsection{Change Point Detection}
A fundamental tool in time series data analysis is Change Point Detection (CPD). It refers to the task of finding when the model generating the values has changed. 
They can generally be categorized into two main groups: online methods that process the data in real time and offline methods that start processing the data after receiving all the values  \cite{Truong2018ChangePointSurvey}. 
In this paper we only look at offline approaches to stay relevant.


This problem has been tackled from various perspectives. There are hundreds of papers in the literature contributing in this widely used field over the past decades. \cite{chen2011parametric, hasan2014information, hsu1982bayesian, lee2017implicit, oh2002analyzing, ramos2016anomalies, reeves2007review, rosenfield2010change, wang2011non, xie2013sequential, yamanishi2004line, Lavielle1999} Bayesian models focus on finding changes in parameters of underlying distributions generating the data \cite{Lee2018TimeSeriesSegmentation, adams2007bayesian, bai1997estimation, barry1993bayesian, erdman2008fast, ray2002bayesian}. Several methods have used penalty function based methods to find models best fitting each segment of the signals \cite{Lavielle1999, lavielle2005using, keshavarz2018optimal, pein2017heterogeneous}. With a slight change in the penalty function, they are usually usable for detecting known and unknown number of change points. 

Ives and Dakos utilized locally linear models and used statistical significance test to determine at which point the changes in model parameters are large enough to signal a change in the state \cite{Ives2012}. Blythe et al. used subspace analysis to reduce data dimensionality to keep the most non-stationary dimensions. This process helps detecting change points more effectively \cite{Blythe2012}. 
%The review: \cite{Truong2018ChangePointSurvey}



\subsection{Deep Neural Networks on Time Series data}
Deep neural networks have shown great empirical performance in recent years in a multitude of tasks. 
Analysing sequences of data is not an exception. Inspired by nature, convolutional neural network(CNN)s can learn to find features in a multi dimensional input while being less sensitive to the exact location of the feature in the input. \cite{lecun2015deep} Recurrent neural network(RNN)s introduce an element of memory into the game rendering them quite powerful in machine translation, time series prediction, and time series classifiction  \cite{cho2014learning, zhang2000predicting, wang2017time, murad2017deep, yang2015deep, Ordonez2016}. They can capture long-term temporal dependencies which is quite useful in our approach. \cite{Che2018}

Murad et al. \cite{murad2017deep} have shown deep RNNs outperform CNNs and deep belief networks in human activity recognition (HAR) task. HAR is quite close to the subject of our paper in the sense that they both take in a multivariate time series data (from sensor readings) and output the intention (internal state) of the system that generated those readings. 


Hybrid models are the combination of some deep architectures \cite{wang2019deep}, say a CNN + RNN or a CNN + a fully connected net. Morales et al. have shown the former preforms better than the latter in HAR\cite{morales2016deep}. Yao et al. \cite{deepsense} introduced a CNN + RNN architecture that outperforms the state of the art both in  classification and in regression tasks. Similar results have been shown in other works such as \cite{Ordonez2016, singh2017transforming, zheng2016exploiting} as well.
